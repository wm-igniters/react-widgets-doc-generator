# AI Provider Configuration
# Choose: 'claude' | 'openai' | 'ollama'
AI_PROVIDER=claude

# AI Model (optional - uses provider default if not set)
# Claude: claude-sonnet-4-0 (recommended), claude-opus-4-0, claude-3-7-sonnet-20250219, claude-3-5-sonnet-20240620, claude-3-haiku-20240307
# OpenAI: gpt-4-turbo-preview, gpt-4, gpt-3.5-turbo
# Ollama: llama2, codellama, mistral (requires local Ollama installation)
AI_MODEL=claude-3-7-sonnet-20250219

# API Keys (only set the one you're using)
ANTHROPIC_API_KEY=your-anthropic-api-key-here
OPENAI_API_KEY=your-openai-api-key-here

# Storybook Path (where to save generated .auto.md files)
STORYBOOK_PATH=../react-widgets-storybook/wm-react-components
COMPONENTS_SOURCE_PATH=../wavemaker-react-runtime/src/components

# Batch Size for parallel LLM processing (default: 5)
# Higher values = faster but may hit API rate limits
# Recommended: 5-10
BATCH_SIZE=5
